{"cells":[{"cell_type":"code","source":["from datetime import datetime\nfrom decimal import Decimal\nfrom typing import List\nfrom pyspark.sql.types import DateType, DecimalType, IntegerType, StructType, StructField, StringType, TimestampType\nfrom pyspark.sql import SparkSession\nimport json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d194ba20-3e08-4e23-92c9-c65ec138a3e0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_dir_content(ls_path):\n  dir_paths = dbutils.fs.ls(ls_path)\n  subdir_paths = [get_dir_content(p.path) for p in dir_paths if p.isDir() and p.path != ls_path]\n  flat_subdir_paths = [p for subdir in subdir_paths for p in subdir]\n  return list(map(lambda p: p.path, dir_paths)) + flat_subdir_paths"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e394f04c-6534-4857-a977-f25b32aebe1d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def common_event(trade_dt: DateType, rec_type: StringType, symbol: StringType, exchange: StringType,\n                 event_tm: TimestampType, event_seq_nb: IntegerType, arrival_tm: TimestampType,\n                 trade_pr: DecimalType(30, 15), trade_size: IntegerType, bid_pr: DecimalType(30, 15),\n                 bid_size: IntegerType, ask_pr: DecimalType(30, 15), ask_size: IntegerType, partition: StringType,\n                 line: StringType):\n    \"\"\"Returns common event schema\n\n    Args:\n        ... field and data type for common schema\n        partition: partition key for trade quote or bad T,Q, or B\n        line: used to return bad line\n    Returns:\n        either the bad line or good record as list of values for each field\"\"\"\n\n    if partition == \"B\":\n        return line\n    else:\n        return [trade_dt, rec_type, symbol, exchange,\n                event_tm, event_seq_nb, arrival_tm,\n                trade_pr, trade_size, bid_pr, bid_size, ask_pr, ask_size, partition, line]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e26ec3c6-c190-4387-baa4-a29d69a41980"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def parse_csv(line: str):\n    \"\"\"CSV parser to be used in Spark transformation process\"\"\"\n    record_type_pos = 2\n    record = line.split(',')\n    try:\n        # logic to parse records\n        if record[record_type_pos] == 'T':\n            event = common_event(datetime.strptime(record[0], \"%Y-%m-%d\"), record[2], record[3], record[6],\n                                 datetime.strptime(record[4], '%Y-%m-%d %H:%M:%S.%f'), int(record[5]),\n                                 datetime.strptime(record[1], '%Y-%m-%d %H:%M:%S.%f'), Decimal(record[7]),\n                                 int(record[8]), None, None, None, None, 'T', None)\n            return event\n        elif record[record_type_pos] == 'Q':\n            event = common_event(datetime.strptime(record[0], \"%Y-%m-%d\"), record[2], record[3], record[6],\n                                 datetime.strptime(record[4], '%Y-%m-%d %H:%M:%S.%f'), int(record[5]),\n                                 datetime.strptime(record[1], '%Y-%m-%d %H:%M:%S.%f'), None, None, Decimal(record[7]),\n                                 int(record[8]), Decimal(record[9]), int(record[10]), 'Q', None)\n            return event\n    except Exception as e:\n        # save record to dummy event in bad partition\n        # fill in the fields as None or empty string\n        print(e)\n        return common_event(None, None, None, None, None, None, None, None, None, None, None, None, None, \"B\",line)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b902e1a7-1d94-41a7-882a-65b190da35dd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def parse_json(line: str):\n    record = json.loads(line)\n    record_type = record['event_type']\n    try:\n        # logic to parse records\n        if record_type == \"T\":\n            event = common_event(datetime.strptime(record[\"trade_dt\"], \"%Y-%m-%d\"), record[\"event_type\"],\n                                 record[\"symbol\"], record[\"exchange\"],\n                                 datetime.strptime(record[\"event_tm\"], '%Y-%m-%d %H:%M:%S.%f'),\n                                 int(record[\"event_seq_nb\"]),\n                                 datetime.strptime(record[\"file_tm\"], '%Y-%m-%d %H:%M:%S.%f'), Decimal(record[\"price\"]),\n                                 int(record[\"size\"]), None, None, None, None, \"T\", None)\n            return event\n        elif record_type == 'Q':\n            event = common_event(datetime.strptime(record[\"trade_dt\"], \"%Y-%m-%d\"), record[\"event_type\"],\n                                 record[\"symbol\"], record[\"exchange\"],\n                                 datetime.strptime(record[\"event_tm\"], '%Y-%m-%d %H:%M:%S.%f'),\n                                 int(record[\"event_seq_nb\"]),\n                                 datetime.strptime(record[\"file_tm\"], '%Y-%m-%d %H:%M:%S.%f'), None, None,\n                                 Decimal(record[\"bid_pr\"]), int(record[\"bid_size\"]), Decimal(record[\"ask_pr\"]),\n                                 int(record[\"ask_size\"]), \"Q\", None)\n            return event\n    except Exception as e:\n        print(e)\n        return common_event(None, None, None, None, None, None, None, None, None, None, None, None, None, \"B\", line)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b9b999f-9b02-4fd7-9376-7d716a790840"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def process_file(path, schem):\n    raw = spark.sparkContext.textFile(path)\n    if 'csv' in path:\n        parsed = raw.map(lambda line: parse_csv(line))\n    else:\n        parsed = raw.map(lambda line: parse_json(line))\n    data = spark.createDataFrame(parsed, schema=schem)\n    return data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c6aa22e-4a9c-41ef-8669-aa516a70f52d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["schema = StructType().add(\"trade_dt\", DateType()).add(\"rec_type\", StringType()).add(\"symbol\", StringType())\\\n    .add(\"exchange\", StringType()).add(\"event_tm\", TimestampType()).add(\"event_seq_nb\", IntegerType())\\\n    .add(\"arrival_tm\", TimestampType()).add(\"trade_pr\", DecimalType()).add(\"trade_size\", IntegerType())\\\n    .add(\"bid_pr\", DecimalType()).add(\"bid_size\", IntegerType()).add(\"ask_pr\", DecimalType())\\\n    .add(\"ask_size\", IntegerType()).add(\"partition\", StringType()).add(\"line\", StringType())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da91c81-0ce4-4da1-a919-589fd8578ac5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mount(\n  source = \"wasbs://gcap@gcblobcf2022.blob.core.windows.net\",\n  mount_point = \"/mnt/azuremountgc\",\n  extra_configs = {\"fs.azure.account.key.gcblobcf2022.blob.core.windows.net\":\"h3lPjueXOs8RE37xrWUD70ZNsHp4wUo/BeGWX6FlLieE/8RZlRWn1VGMT/QoqcF7h2KwXS34RoyVYoUIqS7MeA==\"}\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09c167a0-3292-4d69-852a-32506da43598"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4497323250226909>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m dbutils.fs.mount(\n\u001B[0m\u001B[1;32m      2\u001B[0m   \u001B[0msource\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"wasbs://gcap@gcblobcf2022.blob.core.windows.net\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m   \u001B[0mmount_point\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"/mnt/azuremountgc\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m   \u001B[0mextra_configs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m\"fs.azure.account.key.gcblobcf2022.blob.core.windows.net\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\"h3lPjueXOs8RE37xrWUD70ZNsHp4wUo/BeGWX6FlLieE/8RZlRWn1VGMT/QoqcF7h2KwXS34RoyVYoUIqS7MeA==\"\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m )\n\n\u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py\u001B[0m in \u001B[0;36mf_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    379\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__context__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    380\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__cause__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 381\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mexc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    382\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    383\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mf_with_exception_handling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o298.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/azuremountgc; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/azuremountgc\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:739)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/azuremountgc\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:459)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:824)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:605)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:813)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:467)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:99)\n\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:162)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:97)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:96)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:96)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:300)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:259)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:113)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:140)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:140)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:95)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:366)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:460)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:480)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$2(UsageLogging.scala:232)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:212)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:261)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:375)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:366)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:338)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:20)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:94)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:908)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:908)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:829)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:490)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$2(UsageLogging.scala:232)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:212)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:247)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:261)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:247)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:457)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:369)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n","errorSummary":"java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/azuremountgc; nested exception is: ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4497323250226909>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m dbutils.fs.mount(\n\u001B[0m\u001B[1;32m      2\u001B[0m   \u001B[0msource\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"wasbs://gcap@gcblobcf2022.blob.core.windows.net\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m   \u001B[0mmount_point\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"/mnt/azuremountgc\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m   \u001B[0mextra_configs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m\"fs.azure.account.key.gcblobcf2022.blob.core.windows.net\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\"h3lPjueXOs8RE37xrWUD70ZNsHp4wUo/BeGWX6FlLieE/8RZlRWn1VGMT/QoqcF7h2KwXS34RoyVYoUIqS7MeA==\"\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m )\n\n\u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py\u001B[0m in \u001B[0;36mf_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    379\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__context__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    380\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__cause__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 381\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mexc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    382\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    383\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mf_with_exception_handling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o298.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/azuremountgc; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/azuremountgc\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:739)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/azuremountgc\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:459)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:824)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:605)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:813)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:467)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:99)\n\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:162)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:97)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:96)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:96)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:300)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:259)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:113)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:140)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:140)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:95)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:366)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:460)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:480)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$2(UsageLogging.scala:232)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:212)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:261)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:375)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:366)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:338)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:20)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:94)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:908)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:908)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:829)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:490)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$2(UsageLogging.scala:232)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:212)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:247)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:261)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:247)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:457)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:369)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n"]}}],"execution_count":0},{"cell_type":"code","source":["paths = [p for p in get_dir_content(\"/mnt/azuremountgc\") if '.txt' in p]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b16ff18b-0c63-48fc-88d4-5abd91999f6c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for path in paths:\n    df = process_file(path, schema)\n    df.write.partitionBy(\"partition\").mode(\"overwrite\").parquet(\"/mnt/azuremountgc/output\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea43082f-eb09-4c72-8e65-153db97efff3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96053dbe-3b91-46d4-aaf5-697451d3fe24"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"GuidedCapstoneFinal","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4497323250226901}},"nbformat":4,"nbformat_minor":0}
